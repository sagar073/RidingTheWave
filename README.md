# ğŸŒŠ RidingTheWave: SustainLens - Local AI Deployment with NPU Optimization  

Welcome to **SustainLens**! ğŸš€ This guide takes you through our journey of integrating a sustainability classification AI model into the web application. Along the way, we tackled **dependency conflicts**, experimented with **NPU optimization**, and explored **local AI deployment**.  

---

## ğŸ”§ **1. Resolving Dependency Conflicts**  

### ğŸš¨ **The Problem:**  
When setting up the environment, we ran into **dependency conflicts**â€”particularly with Hugging Face's `transformers`, `tokenizer`, `onnx`, and `onnxruntime`.  

### âœ… **How We Fixed It:**  
âœ”ï¸ Iteratively downgraded & upgraded various libraries to find compatible versions.  
âœ”ï¸ Adjusted **Python** and **NumPy** versions to meet ONNX requirements.  

---

## ğŸ—ï¸ **2. Setting Up Local Deployment**  

### ğŸ” **Steps We Took:**  
âœ… Installed **LM Studio** for local LLM deployment.  
âœ… Downloaded models and installed **Ollama**.  
âœ… Successfully ran **LLaMA 3.2 locally**! ğŸ‰  
âœ… Verified integration of LM Studio models into the application using the **OpenAI SDK**.  

---

## ğŸš€ **3. NPU Optimization Efforts**  

### ğŸ”¥ **What We Tried:**  
âœ… Converted the LLM model into an **NPU-optimized format**.  
âœ… Extracted model weights from LM Studio and attempted conversion to ONNX.  
âœ… Installed NP SDK, tested **QNN SDK/QAIRT** for NPU execution.  
âœ… Pulled NPU-compatible models from LM Studioâ€”but still no NPU utilization. ğŸ¤¯  

### âš ï¸ **Challenges We Faced:**  
âŒ `.model` errors.  
âŒ Issues with `ondevice='npu'`.  
âŒ Could not use `QNNExecutionProvider` in ONNX runtime.  
âŒ High **memory, compute, and GPU usage**â€”but no NPU activity!  

### ğŸ› ï¸ **Genie Runtime Attempts:**  
ğŸ“Œ Installed **QAI Hub models** & generated compatible context binaries.  
ğŸ“Œ Upgraded **PyTorch** for `llama_v3_8b_chat_quantized.export`.  
ğŸ“Œ Encountered errors while running **genie-t2t-run**.  

---

## ğŸ¯ **4. The Breakthrough - "Anything LLM"**  

ğŸ‰ **Success!** We finally got **"Anything LLM"** running on the NPU.  

ğŸ”¹ However, by the time we discovered this, we were **too late in the dev cycle** to fully integrate it into SustainLens.  

### ğŸš€ **Final Approach:**  
âœ… Built a **basic OpenAI-based NLP categorization** for local execution (without NPU/RAG).  
âœ… Attempted integration with **Anything LLM**.  
âœ… Followed "Chat App Using Anything LLM" setup.  
ğŸš§ Faced challenges running the repository locally for parallel execution.  

---

## â­ï¸ **Next Steps**  

ğŸ”„ **Optimize** NPU execution for sustainability classification models.  
ğŸ¤ **Improve** integration with Anything LLM for high-performance inference.  
ğŸ”§ **Refine** the deployment process to streamline future development.  

**The journey continues!** ğŸš€ğŸŒ¿  

ğŸ‘¨â€ğŸ’» **Authors**

**karna.j@northeastern.edu
**gehlot.v@northeastern.edu
**pant.e@northeastern.edu
**mittal.sag@northeastern.edu
**jami.ab@northeastern.edu

